# Transfer Learning



## Overview

‘Transformer’ will be our name for the architecture used in the paper that introduced the transformer, Attention Is All You Need.
https://arxiv.org/pdf/1706.03762.pdf

‘OpenAI GPT’ will reference OpenAI’s work with the transformer architecture which they pretrained on a language model and later tested in a number of downstream tasks.
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf

‘BERT’ will reference Google’s paper on which they modified OpenAI GPT’s architecture to outperform its predecessor in most NLP tasks.
https://arxiv.org/pdf/1810.04805.pdf

Understanding BERT Part 2: BERT Specifics
https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73

NLPドメイン

## BERT
Dissecting BERT Part 1: The Encoder
https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3

Dissecting BERT Appendix: The Decoder
https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f
