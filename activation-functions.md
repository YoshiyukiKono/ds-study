# Activation Functions

https://medium.com/shallow-thoughts-about-deep-learning/how-would-we-find-a-better-activation-function-than-relu-4409df217a5c

## Sigmoid

## tanh

## Rectified Linear Unit
